{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c971e19c",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2ce898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import shlex\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc4e85",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de508d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {}\n",
    "documentID = 0\n",
    "path = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89064373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Stop Words\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    for line in f:\n",
    "        stopwords = line.split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720d5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the tokens\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "                documentID += 1\n",
    "                line_tokens = []\n",
    "                for line in f:\n",
    "                    line_tokens = line.split()\n",
    "                    for each in line_tokens:\n",
    "                        if each not in stopwords:\n",
    "                            if each not in tokens:\n",
    "                                tokens[each] = [documentID]\n",
    "                            else:\n",
    "                                tokens[each].append(documentID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f757bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [1, 2, 3],\n",
       " 'new': [1],\n",
       " 'oil': [1],\n",
       " 'linkedin': [2],\n",
       " 'predicts': [2],\n",
       " 'mining': [2],\n",
       " 'statistics': [2],\n",
       " 'bubble': [2],\n",
       " 'burst': [2],\n",
       " 'big': [3],\n",
       " 'future': [3],\n",
       " 'arpan': [3]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7c11cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inverted Index\n",
    "with open(\"index/invertedindex.txt\", \"w\") as f:\n",
    "    for key in sorted(tokens):\n",
    "        f.write(key)\n",
    "        f.write(\" \")\n",
    "        value = \",\".join(str(v) for v in tokens[key])\n",
    "        f.write(value)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf57bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rotate each word to create the permuterm index\n",
    "def rotate(str, n):\n",
    "    return str[n:] + str[:n]\n",
    "\n",
    "# Create Permuterm Index\n",
    "with open(\"index/permutermindex.txt\",\"w\") as f:\n",
    "    keys = tokens.keys()\n",
    "    for key in sorted(keys):\n",
    "        dkey = key + \"$\"\n",
    "        for i in range(len(dkey),0,-1):\n",
    "            out = rotate(dkey,i)\n",
    "            f.write(out)\n",
    "            f.write(\" \")\n",
    "            f.write(key)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59bceed",
   "metadata": {},
   "source": [
    "## Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719445d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a function used for matching the excat queries and not wildcard (eg: 'data', 'future' etc.)\n",
    "def excat_match(term):\n",
    "    output_list=[]\n",
    "    #This is the list contating document IDs having the required word in query\n",
    "    docID = []\n",
    "    docID.append(inverted[term])\n",
    "\n",
    "    temp = []\n",
    "    for x in docID:\n",
    "        for y in x:\n",
    "            temp.append(y)     \n",
    "\n",
    "    temp = [int(x) for x in temp]\n",
    "    documentID = 0\n",
    "    path = \"dataset\"\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            documentID = documentID + 1\n",
    "            with open(os.path.join(path, file)) as f:\n",
    "                for text in f:\n",
    "                    if documentID in temp:\n",
    "                        #print(file + \"\\n\" + text + \"\\n\")\n",
    "                        output_list.append(file + '\\n'+  text)\n",
    "            f.close()\n",
    "    return(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00f08f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will be used for Wildcard Query Processing in case query has more than 1 wildcards (eg: f*t*r etc.)\n",
    "def bitwise_and(A,B):\n",
    "    return set(A).intersection(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the primary function which will be used for querying purpose. It handles all sorts of Wild card queries, namely,\n",
    "#1) X* --> $X \n",
    "#2) *X --> X$*\n",
    "#3) X*Y --> Y$X*\n",
    "#4) X*Y*Z --> (Z$X*) and (Y*)\n",
    "#5) *X* --> can be converted to X* form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a3c4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def querying(query):\n",
    "    output_list=[]\n",
    "    final_result=[]\n",
    "\n",
    "    # Split query and determine it's type\n",
    "    parts = query.split(\"*\")\n",
    "    \n",
    "    #print(parts)\n",
    "\n",
    "    final_result.append('These are the sub parts of the query')\n",
    "    final_result.append(parts)\n",
    "    \n",
    "    #These are the different cases formed depending on the number of wild card characters and their position in our query\n",
    "    if len(parts)==1:\n",
    "        case =0\n",
    "    elif len(parts) == 3:\n",
    "        case = 4\n",
    "    elif parts[1] == \"\":\n",
    "        case = 1\n",
    "    elif parts[0] == \"\":\n",
    "        case = 2\n",
    "    elif parts[0] != \"\" and parts[1] != \"\":\n",
    "        case = 3\n",
    "\n",
    "    #Case 4 is dealt sperately as it has 2 sub queries    \n",
    "    if case == 4:\n",
    "        if parts[0] == \"\":\n",
    "            case = 1\n",
    "    \n",
    "    #We can see which of the above case our query belongs to using this\n",
    "    #print(\"case =\", case)\n",
    "\n",
    "    # Read Inverted Index\n",
    "    inverted = {}\n",
    "    with open(\"index/invertedindex.txt\") as f:\n",
    "        for line in f:\n",
    "            temp = line.split()\n",
    "            val = temp[1].split(\",\")\n",
    "            inverted[temp[0]] = val\n",
    "\n",
    "\n",
    "    # Read Permuterm Index\n",
    "    permuterm = {}\n",
    "    with open(\"index/permutermindex.txt\") as f:\n",
    "        for line in f:\n",
    "            temp = line.split()\n",
    "            permuterm[temp[0]] = temp[1]\n",
    "     \n",
    "    #print(permuterm)\n",
    "\n",
    "    final_result.append('This is our Permuterm Index corresponding to each word')\n",
    "    final_result.append(permuterm)\n",
    "\n",
    "    \n",
    "    #This function will match the prefix of the word/wildcard query to the words in index\n",
    "    def prefix_match(term, prefix):\n",
    "        term_list = []\n",
    "        for tk in term.keys():\n",
    "            if tk.startswith(prefix):\n",
    "                final_result.append('This is the Permuterm Index where wildcard query is matched')\n",
    "                final_result.append(tk)\n",
    "                term_list.append(term[tk])\n",
    "        return term_list\n",
    "\n",
    "    docID = 0\n",
    "    \n",
    "    #This function is used to process query (ie after prefix match, the word and document is extracted where the prefix match has occured)\n",
    "    def process_query(query):    \n",
    "        term_list = prefix_match(permuterm,query)\n",
    "        #print(term_list)\n",
    "        final_result.append('This is out List contating the terms which match our desired query')\n",
    "        final_result.append(term_list)\n",
    "\n",
    "        #This is the list contating document IDs having the required word in query\n",
    "        docID = []\n",
    "        for term in term_list:\n",
    "            docID.append(inverted[term])\n",
    "\n",
    "        temp = []\n",
    "        for x in docID:\n",
    "            for y in x:\n",
    "                temp.append(y)     \n",
    "\n",
    "        temp = [int(x) for x in temp]\n",
    "        documentID = 0\n",
    "        path = \"dataset\"\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                documentID = documentID + 1\n",
    "                with open(os.path.join(path, file)) as f:\n",
    "                    for text in f:\n",
    "                        if documentID in temp:\n",
    "                            #print(file + \"\\n\" + text + \"\\n\")\n",
    "                            output_list.append(file + '\\n'+  text)\n",
    "                f.close()\n",
    "                \n",
    "        final_result.append('This is the final list containing documnets and their ID')\n",
    "        final_result.append(output_list)\n",
    "    \n",
    "    #Queries are processed on the bases of their cases\n",
    "    if case == 0:\n",
    "        pass\n",
    "    elif case == 1:\n",
    "        query = \"$\" + parts[0]\n",
    "        final_result.append('This is how the query will be processed')\n",
    "        final_result.append(query)\n",
    "    elif case == 2:\n",
    "        query = parts[1] + \"$\"\n",
    "        final_result.append('This is how the query will be processed')\n",
    "        final_result.append(query)\n",
    "    elif case == 3:\n",
    "        query = parts[1] + \"$\" + parts[0]\n",
    "        final_result.append('This is how the query will be processed')\n",
    "        final_result.append(query)      \n",
    "    elif case == 4:\n",
    "        queryA = parts[2] + \"$\" + parts[0]\n",
    "        queryB = parts[1]\n",
    "        final_result.append('This is how the query will be processed')\n",
    "        final_result.append([queryA, queryB])\n",
    "\n",
    "    if case != 4:\n",
    "        process_query(query)\n",
    "    elif case == 4:\n",
    "    # queryA Z$X\n",
    "        term_list = prefix_match(permuterm,queryA)\n",
    "        final_result.append('This is out List contating the terms which match our desired queryA')\n",
    "        final_result.append(term_list)\n",
    "\n",
    "        docID = []\n",
    "        for term in term_list:\n",
    "            docID.append(inverted[term])\n",
    "\n",
    "        temp1 = []\n",
    "        for x in docID:\n",
    "            for y in x:\n",
    "                temp1.append(y)     \n",
    "\n",
    "        temp1 = [int(x) for x in temp1]\n",
    "        term_list = prefix_match(permuterm,queryB)\n",
    "        final_result.append('This is out List contating the terms which match our desired queryB')\n",
    "        final_result.append(term_list)\n",
    "\n",
    "        docID = []\n",
    "        for term2 in term_list:\n",
    "            docID.append(inverted[term2])\n",
    "\n",
    "        temp2 = []\n",
    "        for x in docID:\n",
    "            for y in x:\n",
    "                temp2.append(y)       \n",
    "\n",
    "        temp2 = [int(x) for x in temp2]\n",
    "\n",
    "        #Here bitwise function is used to find the common words from both the queries\n",
    "        temp = bitwise_and(temp1,temp2)\n",
    "        final_result.append('This is List contating common term documents for queryA and queryB')\n",
    "        final_result.append(term)\n",
    "        \n",
    "        documentID = 0\n",
    "        path = \"dataset\"\n",
    "        for root, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                documentID = documentID + 1\n",
    "                with open(os.path.join(path, file)) as f:\n",
    "                    for text in f:\n",
    "                        if documentID in temp:\n",
    "                            #print(file + \"\\n\" + text + \"\\n\")\n",
    "                            output_list.append(file + '\\n'+  text)\n",
    "                f.close()\n",
    "                \n",
    "        final_result.append('This is the final list containing documnets and their ID')\n",
    "        final_result.append(output_list)\n",
    "    \n",
    "    \n",
    "    return(output_list, final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e722b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list, final_result= querying('arpan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef63fc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt\\nbig data is the future arpan']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb5615",
   "metadata": {},
   "source": [
    "## Simply Animated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36b1e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list, final_result= querying('a*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f0c3669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc1.txt\\nbig data is the future arpan']\n"
     ]
    }
   ],
   "source": [
    "#Text Animation \n",
    "def animate_text(function):\n",
    "    for x in function:\n",
    "        if type(x)==dict:\n",
    "            for permuterm in x:\n",
    "                print('This is our Permuterm Index corresponding to the word: \\\"' + x[permuterm] + '\\\"')\n",
    "                \n",
    "                print(permuterm)\n",
    "                time.sleep(0.2)\n",
    "                #os.system('clear')\n",
    "                clear_output(wait=True)\n",
    "        else:    \n",
    "            #print(\"\\n\")\n",
    "            print(x)\n",
    "            time.sleep(2)\n",
    "            #os.system(\"clear\") \n",
    "            clear_output(wait=True)\n",
    "\n",
    "        \n",
    "#Main Program Starts Here....\n",
    "animate_text(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afe429",
   "metadata": {},
   "source": [
    "## Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec930970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean(query):\n",
    "    #Firstly queries are splitted \n",
    "    pieces = shlex.split(query)\n",
    "    include, exclude = [], []\n",
    "    for piece in pieces:\n",
    "        \n",
    "        if piece.startswith('-'):\n",
    "            exclude.append(piece[1:])\n",
    "        else:\n",
    "            include.append(piece)\n",
    "     \n",
    "     #print(include)\n",
    "     #print(exclude)\n",
    "     \n",
    "    #Make include_set and exclude_set having documnets where query to be included and excluded respectively.\n",
    "    include_set = set(querying(include[0])[0])\n",
    "    exclude_set = set()\n",
    "    \n",
    "    for x in include[1:]:\n",
    "        include_set= set(querying(x)[0]).intersection(include_set)\n",
    "    \n",
    "    for x in exclude:\n",
    "        exclude_set= set(querying(x)[0]).union(exclude_set)\n",
    "    \n",
    "    #print(include_set)\n",
    "    #print(exclude_set)\n",
    "    \n",
    "    final_set= list(include_set.difference(exclude_set))\n",
    "    \n",
    "    return final_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5b6593f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1.txt\\nbig data is the future arpan',\n",
       " 'doc3.txt\\nlinkedin predicts data mining and statistics bubble burst']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean('d* -o*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493837c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
